---
title: "README"
author: "Ben Ogorek"
date: "April 1, 2017"
output: md_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


glmmplus: generalized linear mixed models, model selection, and multiple imputation
=======================================================

[![Build Status](https://travis-ci.org/baogorek/glmmplus.svg?branch=master)](https://travis-ci.org/glmmplus)

[![Coverage Status](https://img.shields.io/codecov/c/github/baogorek/glmmplus/master.svg)](https://codecov.io/github/baogorek/glmmplus?branch=master)

## Overview

Missing data is a key feature of nearly any modern regression analysis, but
many of R's flagship modeling tools default to ejecting rows. 
The glmmplus package uses Wald p-values for estimation and model selection in
conjunction with the lme4, nlme, and stats packages to enable consistent 
application of Multiple imputation ([Rubin, 1983][1]), a generally accepted and
principled method of incorporating additional uncertainty into an analysis with
imputation. As the mice package ([van Buuren and Groothuis-Oudshoorn, 2011][2])
is a popular and trusted implementation of multiple imputation in R, glmmplus
builds upon the mids (multiply imputed data set) interface, allowing mids
objects and data frames to be used interchangeably.

The glmmplus package provides sequential model selection of fixed effects
using Rubin's adjusted p-values. Though having fallen out of favor since
the popularization of LASSO ([Tibshirani 1996][3]), sequential
variable selection methods based on p-values have the advantage that
they seamlessly work with the adjusted p-values from multiple imputation,
and easily allow for grouped terms and hierarchies to be considered.
Thus, a large part of glmmplus's code is creating sequential
variable selection routines based on p-values.

This package was originally developed within Google's People Analytics
department, so the output has been designed to satisfy a social scientist's
expectations. Additionally, it follows the Google style guide. While the
original version is still up at Google's github repo (google/glmmplus),
there is no one on the other side to take pull requests and this version
is dated, full of bugs, and should not be used.

## Installation

An easy way to install glmmplus is via the devtools package:

```{r eval = FALSE}
devtools::install_github("baogorek/glmmplus")
```


After installing, load the glmmplus package. The package comes with the
`testdata` data frame that is useful for demonstrating package functionality.
```{r}
library(glmmplus)
# A sample data set with missing values
head(testdata, 3)
```

## Creating multiply imputed data sets

glmmplus provides a light wrapper around the mice package's `mice` function
that offers a single benefit: an argument called "droplist" that
allows the user to specify variables that will not be used in the imputation
models. This is because factor variables with many levels often slow down the
imputation process to unacceptable levels.

```{r, results='hide'}
my.mids <- ImputeData(testdata, m = 5, maxit = 5,
                      droplist = c("factor.1", "factor.2"))
```

The `complete` function has been exported from the mice package, and is used
below to create a data frame with no missing values based on a single
imputation.
```{r}
# a single imputation
complete.df <- complete(my.mids, 1)
head(complete.df, 3)
```

The model fitting functions in glmmplus wrap functions including `base::glm`,
`lme4::glmer`, and `nlme::lme` to provide a common interface to generalized
linear mixed modeling (including the cases where there is no random factor).
The output of these functions have output class called "gfo,"
which stands for "generalized fitted object."

## Forward selection with spline terms 

Below is an example of a forward selection procedure on a data set with
no missing values. Note how the grouped term is associated with a single
p-value of the joint hypothesis.
```{r}
gfo.complete <- ForwardSelect(y.binary ~ ns(x, df = 2) + w + z,
                              family = binomial, data = complete.df)
summary(gfo.complete)
```

Note that the family argument is currently set up to take functions and not the
usual string argument ([Issue 2](https://github.com/baogorek/glmmplus/issues/2)).

A mids object can be used in place of the data frame above,
and all inference is done on the Rubin's Rules adjusted p-values.

```{r}
gfo.mi <- ForwardSelect(y.binary ~ ns(x, df = 2) + w + z,
                             family = binomial, data = my.mids)
summary(gfo.mi)
```

## Incorporating random effects

Adding a random effect via lme4's syntax will automatically switch the
model fitting procedure to use ```glmer```, and it works with mids objects as
well:
```{r}
# Backwards elimination for mixed (fixed and random) models
ran.eff.gfo <- ForwardSelect(y.binary ~ (1 | factor.1) + x + w + z,
                             family = binomial, data = my.mids)
summary(ran.eff.gfo)
```

## Incorporating within-subject time series structures

For a time-series model to be applied within the factor levels, add the argument
`ts.model = "ar1"` to the glmmplus model fitting functions. The model fitting
procedure then changes to `nlme::lme` and an autoregressive structure will be applied
within the single factor level provided. In accordance with the `nlme::lme`
function, there can only be one random effects factor when using this option.
Additionally, this is only possible for gaussian response variables.
time-series structures are planned ([Issue 3](https://github.com/baogorek/glmmplus/issues/3)).

```{r}
ran.eff.ts.gfo <- ForwardSelect(y ~ (1 | factor.1) + x + w + z,
                                family = gaussian, data = my.mids,
                                ts.model = "ar1")
summary(ran.eff.ts.gfo)
```

If you prefer backwards sequential selection, then use the `BackwardEliminate'
function:
```{r}
ran.eff.ts.gfo.be <- BackwardEliminate(y ~ (1 | factor.1) + x + w + z,
                                       family = gaussian, data = my.mids,
                                       ts.model = "ar1")
summary(ran.eff.ts.gfo.be)
```

## Fitting a model without variable selection
Or, if you'd prefer not to use sequential variable selection methods at all,
use the `FitModel` function:

```{r}
ran.eff.ts.gfo.all <- FitModel(y ~ (1 | factor.1) + factor.2 + x + w + z,
                               family = gaussian, data = my.mids,
                               ts.model = "ar1")
summary(ran.eff.ts.gfo.all)
```

Notice that the example above also uses a fixed factor, where like the spline
term in example above, only a single p-value from the grouped hypothesis test
is presented.

# References

[1]: Rubin, D. B. (1987). Multiple Imputation for Nonresponse in Surveys. New York: Wiley

[2]: Stef van Buuren, Karin Groothuis-Oudshoorn (2011). mice: Multivariate Imputation by Chained Equations in R.  Journal of Statistical Software, 45(3), 1-67. URL http://www.jstatsoft.org/v45/i03/.

[3]: Tibshirani, Robert (1996). Regression Shrinkage and Selection via the Lasso. Journal of the Royal Statistical Society. Series B (Methodological), Volume 58, Issue 1, 267-288.